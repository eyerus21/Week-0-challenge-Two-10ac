{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import STOPWORDS,WordCloud\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import string\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDwep1K8Erxl"
   },
   "source": [
    "**Project:** Data Minining Project for  X company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzIu-UWIDXHw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7-ii3uyI8KY"
   },
   "source": [
    "The CRISP-DM Framework\n",
    "\n",
    "\n",
    "The CRISP-DM methodology provides a structured approach to planning a data mining project. It is a robust and well-proven methodology.\n",
    "* Business understanding (BU): Determine Business Objectives, Assess Situation, Determine Data Mining Goals, Produce Project Plan\n",
    "\n",
    "* Data understanding (DU): Collect Initial Data, Describe Data, Explore Data, Verify Data Quality\n",
    "\n",
    "* Data preparation (DP): Select Data, Clean Data, Construct Data, Integrate Data\n",
    "\n",
    "* Modeling (M): Select modeling technique, Generate Test Design, Build Model, Assess Model\n",
    "*  Evaluation (E): Evaluate Results, Review Process, Determine Next Steps\n",
    "*  Deployment (D): Plan Deployment, Plan Monitoring and Maintenance, Produce Final Report, Review Project\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "[What is the CRISP-DM methodology?](https://www.sv-europe.com/crisp-dm-methodology/)\n",
    "\n",
    "[Introduction to CRISP DM Framework for Data Science and Machine Learning](https://www.linkedin.com/pulse/chapter-1-introduction-crisp-dm-framework-data-science-anshul-roy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lo7Ml7tMQOf"
   },
   "source": [
    "**Data Set**\n",
    "### The data is for company X which is trying to control attrition. \n",
    "### There are two sets of data: \"Existing employees\" and \"Employees who have left\". The following attributes are available for every employee.\n",
    "\n",
    "\n",
    "*   Satisfaction Level\n",
    "\n",
    "*   Last evaluation\n",
    "\n",
    "*   Number of projects\n",
    "\n",
    "*   Average monthly hours\n",
    "\n",
    "*   Time spent at the company\n",
    "*   Whether they have had a work accident\n",
    "\n",
    "\n",
    "*  Whether they have had a promotion in the last 5 years\n",
    "\n",
    "\n",
    "*   Departments (column sales)\n",
    "\n",
    "\n",
    "*   Salary\n",
    "\n",
    "\n",
    "*  Whether the employee has left\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjSj2A2sSph_"
   },
   "source": [
    "**Your Role**\n",
    " \n",
    "\n",
    "*   As data science team member X company asked you to answer this two questions.\n",
    "*  What type of employees is leaving? \n",
    "\n",
    "*   Determine which employees are prone to leave next.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajdEVA7LiBUp"
   },
   "source": [
    "Business Understanding\n",
    "\n",
    "---\n",
    "\n",
    "This step mostly focuses on understanding the Business in all the different aspects. It follows the below different steps.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Identify the goal and frame the business problem.\n",
    "* Prepare Analytical Goal i.e. what type of performance metric and loss function to use\n",
    "* Gather information on resource, constraints, assumptions, risks etc\n",
    "* Gather information on resource, constraints, assumptions, risks etc\n",
    "*   Prepare Work Flow Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4MwiCYzj2_u"
   },
   "source": [
    "### Write the main objectives of this project in your words?\n",
    "minimum of 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "STyLda45j1Mf"
   },
   "outputs": [],
   "source": [
    "main_objectives =''' \n",
    "Identify the goal and frame the business problem.\n",
    "Prepare Analytical Goal i.e. what type of performance metric and loss function to use\n",
    "Gather information on resource, constraints, assumptions, risks etc\n",
    "Gather information on resource, constraints, assumptions, risks etc\n",
    "Prepare Work Flow Chart\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CuOlxLxKMOLI"
   },
   "outputs": [],
   "source": [
    "assert len(main_objectives) > 100 \n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(main_objectives) > 80 \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyXeNxlCkbaw"
   },
   "source": [
    "### Outline the different data analysis steps you will follow to carry out the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rC-tl8sUksQq"
   },
   "outputs": [],
   "source": [
    "dm_outline = '''\n",
    "To improve your data analysis skills and simplify your decisions, execute these five steps in your data analysis process:\n",
    "Step 1: Define Your Questions. ...\n",
    "Step 2: Set Clear Measurement Priorities. ...\n",
    "Step 3: Collect Data. ...\n",
    "Step 4: Analyze Data. ...\n",
    "Step 5: Interpret Results.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-K1mWuDoksTk"
   },
   "outputs": [],
   "source": [
    "assert len(dm_outline) > 100 \n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(dm_outline) > 70 \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmUDFG1wkzUy"
   },
   "source": [
    "### What metrics will you use to measure the performance of your data analysis model? \n",
    "Write the equations of the metrics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCNulojKk_BP"
   },
   "source": [
    "e.g. Precision = $\\frac{TP}{(TP + FP)}$\n",
    "\n",
    "\n",
    "\n",
    "F1 Score\n",
    "F1= 2*Precision*Recall/precision+Recall\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLS2YHoRk_EK"
   },
   "source": [
    "Why do you choose these metrics? minimum of 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LSynT14KlPSJ"
   },
   "outputs": [],
   "source": [
    "why_metrics = '''F1 Score is needed when you want to seek a balance between Precision and Recall. Rightâ€¦so what is the difference between F1 Score and Accuracy then? accuracy can be largely contributed by a large number of True Negatives which in most business circumstances, we do not focus on much whereas False Negative and False Positive usually has business costs (tangible & intangible) thus F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yr-Mk0E8lPVJ"
   },
   "outputs": [],
   "source": [
    "assert len(why_metrics) > 100 \n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(why_metrics) > 80 \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAo19Ip6lUtm"
   },
   "source": [
    "### How would you know if your data analysis work is a success or not?\n",
    "minimum of 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HESsiXW5llX-"
   },
   "outputs": [],
   "source": [
    "how_success = '''Data analysis is supposed to be about the data, right? Just the facts? And for the most part it is, up until the point you need to communicate your findings to an audience. The problem is that in any data analysis that would be meaningful to others, there are simply too many results to present, and so choices must be made. Depending on who the audience is, or who the audience is composed of, you will need to tune your presentation in order to get the audience to accept the analysis. \n",
    "\n",
    "Good communication with an audience can often play a much bigger role in success than whether you used a linear model or quadratic model. Trust between an analyst and audience is critical when an analyst must make choices about what to present and what to omit. Admitting that human nature plays a role in data analysis success is difficult because humans are highly subjective, inconsistent, and difficult to quantify. However, I think doing so gives us a better understanding about how to judge the quality of data analyses and how to improve them in the future.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FdUoiMIOlmXq"
   },
   "outputs": [],
   "source": [
    "assert len(how_success) > 100 \n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(how_success) > 80 \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQE6dqo6l1TZ"
   },
   "source": [
    "## What kind of challenges do you expect in your analysis?\n",
    "List at least 3 challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrAhBQhQl8Lh"
   },
   "outputs": [],
   "source": [
    "challenge_text = ''' 1) The amount of data being collected\n",
    "With todayâ€™s data-driven organizations and the introduction of big data, risk managers and other employees are often overwhelmed with the amount of data that is collected. An organization may receive information on every incident and interaction that takes place on a daily basis, leaving analysts with thousands of interlocking data sets.\n",
    "\n",
    "2) Visual representation of data\n",
    "To be understood and impactful, data often needs to be visually presented in graphs or charts. While these tools are incredibly useful, itâ€™s difficult to build them manually. Taking the time to pull information from multiple areas and put it into a reporting tool is frustrating and time-consuming.\n",
    "\n",
    "3) Scaling data analysis\n",
    "Finally, analytics can be hard to scale as an organization and the amount of data it collects grows. Collecting information and creating reports becomes increasingly complex. A system that can grow with the organization is crucial to manage this issue.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EedHa-Pll8X7"
   },
   "outputs": [],
   "source": [
    "assert len(challenge_text) > 100 \n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(how_success) > 80 \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcJ8M6uWDeSE"
   },
   "source": [
    "<h2>Using the processed twitter data from yesterday's challenge</h2>.\n",
    "\n",
    "\n",
    "- Form a new data frame (named `cleanTweet`), containing columns $\\textbf{clean-text}$ and $\\textbf{polarity}$.\n",
    "\n",
    "- Write a function `text_category` that takes a value `p` and returns, depending on the value of p, a string `'positive'`, `'negative'` or `'neutral'`.\n",
    "\n",
    "- Apply this function (`text_category`) on the $\\textbf{polarity}$ column of `cleanTweet` in 1 above to form a new column called $\\textbf{score}$ in `cleanTweet`.\n",
    "\n",
    "- Visualize The $\\textbf{score}$ column using piechart and barchart\n",
    "\n",
    "<h5>Now we want to build a classification model on the clean tweet following the steps below:</h5>\n",
    "\n",
    "* Remove rows from `cleanTweet` where $\\textbf{polarity}$ $= 0$ (i.e where $\\textbf{score}$ = Neutral) and reset the frame index.\n",
    "* Construct a column $\\textbf{scoremap}$ Use the mapping {'positive':1, 'negative':0} on the $\\textbf{score}$ column\n",
    "* Create feature and target variables `(X,y)` from $\\textbf{clean-text}$ and $\\textbf{scoremap}$ columns respectively.\n",
    "* Use `train_test_split` function to construct `(X_train, y_train)` and `(X_test, y_test)` from `(X,y)`\n",
    "\n",
    "* Build an `SGDClassifier` model from the vectorize train text data. Use `CountVectorizer()` with a $\\textit{trigram}$ parameter.\n",
    "\n",
    "* Evaluate your model on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85WxmGNGDcBY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Challenge_ Day2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
